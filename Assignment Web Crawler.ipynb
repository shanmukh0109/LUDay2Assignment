{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiovascular-induction",
   "metadata": {},
   "source": [
    "# AppScienti Info Systems Private Limited Assignment\n",
    "BY Chintamaneni Shanmukh Chowdary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chintamaneni Shanmukh Chowdary\n",
    "#importing required packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affecting-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls to crawl\n",
    "urls = ['https://www.iplt20.com/stats/all-time/most-sixes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "duplicate-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawling and getting data\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    tables = soup.find(\"table\")\n",
    "    heads = tables.find_all('th')\n",
    "    rows=tables.find_all('td')\n",
    "    columns=[]\n",
    "    rowe=[]\n",
    "    for head in heads:\n",
    "        columns.append(\"\".join(head.text.replace('\\n','').strip()))\n",
    "    for row in rows:\n",
    "        rowe.append(row.text.replace('\\n','').replace(' ',''))\n",
    "    r=[]\n",
    "    for i in range(0, len(rowe), 14): \n",
    "        r.append(rowe[i:i + 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "concerned-taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     POS                PLAYER  Mat Inns  NO  Runs    HS    Avg     BF  \\\n",
      "1      1            ChrisGayle  142  141  16  4965  175*  39.72  3,333   \n",
      "2      2          ABdeVilliers  184  170  40  5162  133*  39.70  3,403   \n",
      "3      3           RohitSharma  213  208  28  5611  109*  31.17  4,303   \n",
      "4      4               MSDhoni  220  193  73  4746   84*  39.55  3,494   \n",
      "5      5         KieronPollard  178  160  51  3268   87*  29.98  2,182   \n",
      "..   ...                   ...  ...  ...  ..   ...   ...    ...    ...   \n",
      "96    96       MoisesHenriques   62   54  18  1000   74*  27.77    788   \n",
      "97    97           RahulDravid   89   82   5  2174   75*  28.23  1,882   \n",
      "98    98  SubramaniamBadrinath   95   67  20  1441   71*  30.65  1,212   \n",
      "99    99       KumarSangakkara   71   68   3  1687    94  25.95  1,392   \n",
      "100  100         ThisaraPerera   37   30   8   422    40  19.18    307   \n",
      "\n",
      "         SR 100  50   4s   6s  \n",
      "1    148.96   6  31  405  357  \n",
      "2    151.68   3  40  413  251  \n",
      "3    130.39   1  40  491  227  \n",
      "4    135.83   0  23  325  219  \n",
      "5    149.77   0  16  212  214  \n",
      "..      ...  ..  ..  ...  ...  \n",
      "96   126.90   0   5   87   28  \n",
      "97   115.51   0  11  269   28  \n",
      "98   118.89   0  11  154   28  \n",
      "99   121.19   0  10  195   27  \n",
      "100  137.45   0   0   23   26  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#Creating dataframe and storing data\n",
    "df=pd.DataFrame(columns=columns,data=r,index=[i[0] for i in r])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incredible-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    POS                PLAYER  Mat  Inns  NO  Runs    HS    Avg     BF  \\\n",
      "0     1            ChrisGayle  142   141  16  4965  175*  39.72  3,333   \n",
      "1     2          ABdeVilliers  184   170  40  5162  133*  39.70  3,403   \n",
      "2     3           RohitSharma  213   208  28  5611  109*  31.17  4,303   \n",
      "3     4               MSDhoni  220   193  73  4746   84*  39.55  3,494   \n",
      "4     5         KieronPollard  178   160  51  3268   87*  29.98  2,182   \n",
      "..  ...                   ...  ...   ...  ..   ...   ...    ...    ...   \n",
      "95   96       MoisesHenriques   62    54  18  1000   74*  27.77    788   \n",
      "96   97           RahulDravid   89    82   5  2174   75*  28.23  1,882   \n",
      "97   98  SubramaniamBadrinath   95    67  20  1441   71*  30.65  1,212   \n",
      "98   99       KumarSangakkara   71    68   3  1687    94  25.95  1,392   \n",
      "99  100         ThisaraPerera   37    30   8   422    40  19.18    307   \n",
      "\n",
      "        SR  100  50   4s   6s  \n",
      "0   148.96    6  31  405  357  \n",
      "1   151.68    3  40  413  251  \n",
      "2   130.39    1  40  491  227  \n",
      "3   135.83    0  23  325  219  \n",
      "4   149.77    0  16  212  214  \n",
      "..     ...  ...  ..  ...  ...  \n",
      "95  126.90    0   5   87   28  \n",
      "96  115.51    0  11  269   28  \n",
      "97  118.89    0  11  154   28  \n",
      "98  121.19    0  10  195   27  \n",
      "99  137.45    0   0   23   26  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#writing data to csv file\n",
    "df.to_csv('Most_Sixes_In_IPL.csv', index=False)\n",
    "mydata = pd.read_csv('Most_Sixes_In_IPL.csv')\n",
    "print(mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-agency",
   "metadata": {},
   "source": [
    "# Working of my basic web crawler\n",
    "AppScienti Info Systems Private Limited\n",
    "Assignment\n",
    "Create a basic web crawler to crawl any particular website and store information in any file format (CSV will be better) .\n",
    "\n",
    "Chintamaneni Shanmukh Chowdary, K L University\n",
    "Phone: 9182476746\n",
    "\n",
    "Working Procedure of my basic web crawler\n",
    "\n",
    "•\tFirst, all required python packages like BeautifulSoup for web crawling, requests for sending and getting HTTP response from the web page, pandas for structuring the data in the form of table using data frame, csv package for creating, writing, reading csv files are imported.\n",
    "\n",
    "•\tThen, a HTTP request was sent to URL (https://www.iplt20.com/stats/all-time/most-sixes) and received the response by using GET method. Then the response is parsed using HTML parser to get HTML content. This content is called as soup.\n",
    "\n",
    "•\tThen, by using the find function by passing “table” as argument, Received the table contents. Then found the table headings in it by  using the find function by passing “th” as argument. Then found the table data in it by  using the find function by passing “td” as argument.\n",
    "\n",
    "•\tThen, this data is stored in the form of table by using the data frame in pandas.\n",
    "\n",
    "•\tFinally, this data is written into the csv file named (Most_Sixes_In_IPL.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
